# ニューラルネットワークの学習
## 学習とは

訓練データから最適な重みパラメータの値を自動で獲得すること。

ニューラルネットワークでは損失関数という指標を用意し、それによって得られた値を可能な限り小さくしていくことを目標とする。

## データ駆動
画像分類において、以下のアプローチが考えられる。

1. 人間が、パラメータなどを設定する
2. 人間が画像を分類する際の着眼点（特徴量）を設定して、それに応じてコンピュータに分類させる
3. コンピュータが特徴量を独自に生み出し、分類する

ニューラルネットワークは3つ目のアプローチを行う。したがって人間の手が直接関わるのは、データのみである。

![Alt text](kikai.png)

## 損失関数
ニューラルネットワークの学習における**指標**

ここでいう指標とは「正解とどのくらい離れているか。どのくらい間違っているか」を表す。 

### 2乗和誤差
損失関数の１つ。以下の式で表される。

$$
\begin{align}
E = \frac{1}{2} \sum_{k}(y_k - t_k)^2
\end{align}
$$

pythonで実装するとこんな感じかな

```python
def sum_squared_error(y, t):
    return 0.5 * np.sum((y - t) ** 2)
```

### 交差エントロピー誤差
損失関数の1つ。以下の式で表される。

$$
\begin{align}
E = - \sum_{k}t_k \log y_k
\end{align}
$$

pythonでの実装

```python
def cross_entropy_error(y, t):
    delta = 1e-7
    return -np.sum(t * np.log(y + delta))
```

## ミニバッチ学習
機械学習における学習とは、訓練データに対する損失関数を求め、その値をできるだけ小さくするようなパラメータを探し出すということである。
訓練データが100個くらいなら、全てのデータに対する損失関数を求め、その平均を算出することで学習における指標を用意することができる。
例えばデータが$N$個なら、以下のような式となる。

$$
E = -\frac{1}{N}\sum_{n}\sum_{k}t_{nk}\log{y_{nk}}
$$

しかし、$N$が巨大になった場合も同様なフローを実行できるかと言われたら難しい。
そこで、数あるデータのうちいくつか、例えば100枚を無作為に選び出して、その100枚を使って学習を行うという方法がある。その方法をミニバッチ学習という。

実装は`minibatch.ipynb`